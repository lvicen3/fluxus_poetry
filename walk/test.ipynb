{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2f3052bc080d8c9694360701bcc4d8d0a5d52514d6869db6f6eab72cbb0f4158"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a document into a list of tokens\n",
    "# Arguments:\n",
    "# doc: A string containing input document\n",
    "# Returns: tokens (list)\n",
    "# Where, tokens (list) is a list of tokens that the document is split into\n",
    "def get_tokens(doc):\n",
    "\ttokens = re.split(r\"[^A-Za-z0-9-']\", doc)\n",
    "\ttokens = list(filter(len, tokens))\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_name = 'Radiohead'\n",
    "random.seed(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull discography table from band's wiki\n",
    "url = 'https://en.wikipedia.org/wiki/{0}_discography'.format(band_name)\n",
    "page = requests.get('https://en.wikipedia.org/wiki/{0}_discography'.format(band_name))\n",
    "discography_soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SONG:  the-gloaming\n"
     ]
    }
   ],
   "source": [
    "album_list = discography_soup.find(attrs={'id':'Studio_albums'}).findNext('table', attrs={'class': 'wikitable plainrowheaders'}).find_all('th',attrs={'scope':'row'})\n",
    "\n",
    "album_urls = []\n",
    "for album in album_list:\n",
    "    album_urls.append('https://en.wikipedia.org/'+album.find('a')['href'])\n",
    "album_url = random.choice(album_urls)\n",
    "\n",
    "# pull album page -> get a song\n",
    "album_page = requests.get(album_url)\n",
    "album_soup = BeautifulSoup(album_page.content,'html.parser')\n",
    "songs = album_soup.find('table',attrs={'class':'tracklist'}).find_all(attrs={'style':'vertical-align:top'})\n",
    "song = random.choice(songs).get_text()\n",
    "song = re.sub(r'\\([^)]*\\)','',song)\n",
    "song = song.lower()\n",
    "# song = song.replace('\"','').lower().replace(' ','-')\n",
    "song = get_tokens(song)\n",
    "parsed_song = ''\n",
    "for word in song[:-1]:\n",
    "    parsed_song+=word + '-'\n",
    "parsed_song+=song[-1]\n",
    "print('SONG: ',parsed_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://genius.com/Radiohead-the-gloaming-lyrics\n"
     ]
    }
   ],
   "source": [
    "# build url for genius website\n",
    "# parsed_song = ((re.search('\"(\\w[\\s\\w]*)\"', song).group(1)).lower()).replace(' ','-')\n",
    "parsed_band = band_name.lower().capitalize().replace('_','-')\n",
    "genius_url = 'https://genius.com/' + parsed_band + '-' + parsed_song + '-lyrics'\n",
    "print(genius_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Verse 1]\nGenie let out of the bottle\nIt is now the witching hour\nGenie let out of the bottle\nIt is now the witching hour\nMurderers, you're murderers\nWe are not the same as you\nGenie let out of the bottle\nFunny, ha-ha, funny how\nWhen the walls bend, when the walls bend\nWith your breathing, with your breathing\nWhen the walls bend, when the walls bend\nWith your breathing, with your breathing\nWith your breathing\n\n[Verse 2]\nThey will suck you down to the other side\nThey will suck you down to the other side\nThey will suck you down to the other side\nThey will suck you down to the other side\nTo the shadows blue and red, shadows blue and red\nYour alarm bells, your alarm bells\nShadows blue and red, shadows blue and red\nYour alarm bells, your alarm\nThey should be ringing, they should be ringing\nThey should be ringing, they should be ringing\nThey should be ringing, they should be ringing\nThey should be ringing, they should be ringing\nThey should be ringing, they should be ringing\nThey should be ringing, they should be ringing\nThis is the gloaming\n"
     ]
    }
   ],
   "source": [
    "# page = requests.get('https://genius.com/Vampire-weekend-hannah-hunt-lyrics')\n",
    "page = requests.get(genius_url)\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "lyrics = soup.find('p')\n",
    "print(lyrics.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "When the walls bend, when the walls bend\n"
     ]
    }
   ],
   "source": [
    "lyrics_str = lyrics.get_text()\n",
    "lyrics_str = re.sub(r'\\[[^]]*\\][\\n\\r]','',lyrics_str)\n",
    "lines = re.split(r'[\\n\\r]+', lyrics_str)\n",
    "print(random.choice(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['https://images.unsplash.com/photo-1565174413910-7d55c5a23361?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&w=1000&q=80', 'https://images.unsplash.com/photo-1536012618-fc362da6629f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&w=1000&q=80', 'https://images.unsplash.com/photo-1576456344355-eaa41dda10ad?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&w=1000&q=80', 'https://images.unsplash.com/photo-1565174413910-7d55c5a23361?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&w=1000&q=80', 'https://images.unsplash.com/photo-1576456344355-eaa41dda10ad?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&w=1000&q=80', 'https://images.unsplash.com/photo-1536012618-fc362da6629f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&w=1000&q=80']\nhttps://images.unsplash.com/photo-1565174413910-7d55c5a23361?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&w=1000&q=80\n"
     ]
    }
   ],
   "source": [
    "def get_art(words=['a','b','c']):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    url = 'https://unsplash.com/s/photos/{0}-{1}-{2}'.format(words[0],words[1],words[2])\n",
    "    page = requests.get(url, headers = headers)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    figures = soup.findAll('figure')\n",
    "    img_links = []\n",
    "    for figure in figures:\n",
    "        try:\n",
    "            img_links.append(figure.find('img').get('src'))\n",
    "        except:\n",
    "            ()        \n",
    "    \n",
    "    img_links = [img_link for img_link in img_links if re.match(r'(.+)photo(.+)', img_link)]\n",
    "    # img_links = soup.findAll('a',attrs={'class':'Link-oxrwcw-0 lcvbOy'})\n",
    "\n",
    "    print(img_links)\n",
    "    return random.choice(img_links)\n",
    "\n",
    "print(get_art())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}